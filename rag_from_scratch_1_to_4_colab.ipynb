{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixNFNepfBH6u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import bs4\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "import tiktoken\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up environment variables for API keys and tracing\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"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\"\n",
        "os.environ['HUGGINGFACE_API_KEY'] = \"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\""
      ],
      "metadata": {
        "id": "V0n4ctw_FWMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### INDEXING ####\n",
        "# Load Documents\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "sbWBfPqxFax6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the document into smaller chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "splits = text_splitter.split_documents(docs[:2])  # Process smaller dataset initially\n",
        "\n",
        "# Load Hugging Face model for embeddings\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embedding_function = HuggingFaceEmbeddings(model_name=model_name)\n",
        "\n",
        "# Store documents in Chroma using the embedding function\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding_function\n",
        ")\n",
        "\n",
        "# Create a retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
        "print(\"Retriever created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkDQPo3TFiKA",
        "outputId": "4f274255-efc3-46bc-c7d6-3a4c72635e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-f23ab9652a37>:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
            "  embedding_function = HuggingFaceEmbeddings(model_name=model_name)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retriever created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### RETRIEVAL ###\n",
        "question = \"What kinds of pets do I like?\"\n",
        "document = \"My favorite pet is a cat.\"\n",
        "\n",
        "# Tokenization function using tiktoken\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "# Example usage of the tokenization function\n",
        "tokens = num_tokens_from_string(question, \"cl100k_base\")\n",
        "print(f\"Number of tokens in the question: {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFU9QCl3Ftdz",
        "outputId": "a790971d-8ec8-48d4-8e39-d24990f6b4a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens in the question: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "### GENERATION ###\n",
        "# Retrieve relevant documents based on the query\n",
        "retrieved_docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")\n",
        "\n",
        "# Prompt template for the LLM\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# Initialize the Hugging Face model for text generation with increased max_length\n",
        "generation_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=\"gpt2\",\n",
        "    max_length=200,  # Increase the max_length to accommodate longer inputs\n",
        "    pad_token_id=50256  # Set the pad_token_id to eos_token_id\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=generation_pipeline)\n",
        "\n",
        "# Create a chain combining the prompt and LLM\n",
        "chain = prompt | llm\n",
        "\n",
        "# Run the chain with the retrieved documents and question\n",
        "response = chain.invoke({\n",
        "    \"context\": retrieved_docs[0].page_content,\n",
        "    \"question\": \"What is Task Decomposition?\"\n",
        "})\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO9IYkwdF0vE",
        "outputId": "d48ae682-5766-4bf8-f90c-07e2fcd06ce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Answer the question based only on the following context:\n",
            "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
            "\n",
            "Question: What is Task Decomposition?\n",
            "\n",
            "Answer:\n",
            "\n",
            "Task decomposition is the transformation of tasks into tasks which they do, e.g. using task-specific instructions (for example: when a task in memory completes, it would complete the task that was executed earlier in the process in order to get to the next point in task). The term task-convergence is typically used from the perspective of human programmers (e.g. \"Where do I start?\") In human programming, every task in memory is equivalent to a task\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### RAG CHAIN ###\n",
        "# Pull a RAG prompt template from LangChain's hub (Optional)\n",
        "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# Create a RAG chain combining retriever, prompt, and LLM\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Run the RAG chain\n",
        "rag_response = rag_chain.invoke(\"What is Task Decomposition?\")\n",
        "print(rag_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZihOujjF6H5",
        "outputId": "d40827a5-a92b-4642-ea39-98b9115333e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:5434: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
            "  prompt = loads(json.dumps(prompt_object.manifest))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Answer the question based only on the following context:\n",
            "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')]\n",
            "\n",
            "Question: What is Task Decomposition?\n",
            "\n",
            "Why are task decompositions performed? (1)\n",
            "\n",
            "The reason task decompositions are implemented (for each task) is that every time a task is completed, the processor writes to the local file system (that's what a file can do) the steps, and this will automatically be removed. (2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gfw42rYKIiz2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}