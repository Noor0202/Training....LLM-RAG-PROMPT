{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceHub\n",
    "import os\n",
    "llm_hugging_face = HuggingFaceHub(huggingfacehub_api_token=os.environ[\"HUGG_API_KEY\"],repo_id = \"google/flan-t5-large\",model_kwargs={\"temperature\":0.5,\"max_length\":120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love\n"
     ]
    }
   ],
   "source": [
    "def make_openai_request(prompt):\n",
    "    try:\n",
    "        return llm_hugging_face.predict(prompt)\n",
    "    except RateLimitError as e:\n",
    "        return \"Rate limit exceeded. Waiting to retry...\"\n",
    "\n",
    "# response = make_openai_request(\"What is the capital of India?\")\n",
    "# print(response)\n",
    "response = make_openai_request(\"please write a poem on AI\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calcutta\n"
     ]
    }
   ],
   "source": [
    "#simple chain - \n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define a prompt template\n",
    "prompt_template = PromptTemplate(input_variables=[\"question\"], template=\"What is the answer to {question}?\")\n",
    "\n",
    "# Define the LLM chain\n",
    "llm_chain = LLMChain(llm=llm_hugging_face, prompt=prompt_template)\n",
    "# print(llm_chain)\n",
    "# Pass input to the chain\n",
    "response = llm_chain.run(\"What is the capital of India?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a field that has been hailed as a breakthrough in the field of computer science. It is a field that has been hailed as a breakthrough in the field of computer science. It is a field that has been hailed as a breakthrough in the field of computer science. It is a field that has been hailed as a breakthrough in the field of computer science. It is a field that has been hailed as a breakthrough in the field of computer science. It is a field that has been \n"
     ]
    }
   ],
   "source": [
    "#sequencial chain - \n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# Step 1: Summarize the topic\n",
    "summarize_template = PromptTemplate(input_variables=[\"topic\"], template=\"Summarize the topic: {topic}\")\n",
    "summarize_chain = LLMChain(llm=llm_hugging_face, prompt=summarize_template)\n",
    "\n",
    "# Step 2: Expand the summary into an essay\n",
    "essay_template = PromptTemplate(input_variables=[\"summary\"], template=\"Write an essay based on the summary: {summary}\")\n",
    "essay_chain = LLMChain(llm=llm_hugging_face, prompt=essay_template)\n",
    "\n",
    "# Sequential chain combining both steps\n",
    "sequential_chain = SimpleSequentialChain(chains=[summarize_chain, essay_chain])\n",
    "\n",
    "# Run the chain\n",
    "response = sequential_chain.run(\"Artificial Intelligence\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import TransformChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Function to determine which prompt to use\n",
    "def select_prompt(inputs):\n",
    "    question = inputs['question']\n",
    "    if len(question) > 10:\n",
    "        return {\"selected_prompt\": \"Generate a detailed answer: \" + question}\n",
    "    else:\n",
    "        return {\"selected_prompt\": \"Generate a short answer: \" + question}\n",
    "\n",
    "# Create a Transform Chain\n",
    "transform_chain = TransformChain(input_variables=[\"question\"], output_variables=[\"selected_prompt\"], transform=select_prompt)\n",
    "\n",
    "# Create LLMChain with dynamic prompt selection\n",
    "dynamic_chain = LLMChain(llm=llm_hugging_face, prompt=PromptTemplate(input_variables=[\"selected_prompt\"], template=\"{selected_prompt}\"))\n",
    "\n",
    "# Use a Sequential Chain to combine both\n",
    "conditional_chain = SimpleSequentialChain(chains=[transform_chain, dynamic_chain])\n",
    "\n",
    "# Test with different inputs\n",
    "response = conditional_chain.run(\"What is AI?\")\n",
    "print(response)\n",
    "\n",
    "response = conditional_chain.run(\"Can you explain artificial intelligence in detail?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98\n",
      "Nationals\n",
      " 97\n",
      "Reds\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "file_path = (\"mlb_teams_2012.csv\")\n",
    "\n",
    "# loader = CSVLoader(file_path=file_path)\n",
    "# data = loader.load()\n",
    "\n",
    "# for record in data[:2]:\n",
    "#     print(record)\n",
    "    \n",
    "loader = CSVLoader(file_path=file_path, source_column=\"Team\")\n",
    "\n",
    "data = loader.load()\n",
    "for record in data[:2]:\n",
    "    print(record.page_content.split(\"\\n\")[2].split(\":\")[1])\n",
    "    print(record.metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load example document with encoding specified\n",
    "with open(\"state_of_the_union.txt\", encoding=\"utf-8\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "\n",
    "# Print the first two chunks\n",
    "# print(len(texts[0].page_content))\n",
    "# print(texts[1].page_content)\n",
    "# print(texts[2].page_content)\n",
    "# print(texts[3].page_content)\n",
    "# print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHANOOR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "C:\\Users\\SHANOOR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "raw_documents = TextLoader('state_of_the_union.txt',encoding=\"utf-8\").load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Initialize the embedding model from Hugging Face\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHANOOR\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_core\\_api\\deprecation.py:151: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'embed_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m db \u001b[38;5;241m=\u001b[39m Chroma(collection_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_collection\u001b[39m\u001b[38;5;124m\"\u001b[39m, embedding_function\u001b[38;5;241m=\u001b[39membedding_model\u001b[38;5;241m.\u001b[39mencode)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Add the documents and embeddings to the vector store\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\langchain_community\\vectorstores\\chroma.py:277\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[1;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 277\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m(texts)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'embed_documents'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Function to convert documents into embeddings\n",
    "def embed_documents(documents):\n",
    "    texts = [doc.page_content for doc in documents]\n",
    "    embeddings = embedding_model.encode(texts)\n",
    "    return embeddings\n",
    "\n",
    "embeddings = embed_documents(documents)\n",
    "\n",
    "\n",
    "# Create the Chroma vector store\n",
    "db = Chroma(collection_name=\"my_collection\", embedding_function=embedding_model.encode)\n",
    "\n",
    "# Add the documents and embeddings to the vector store\n",
    "db.add_texts(texts=[doc.page_content for doc in documents], metadatas=[doc.metadata for doc in documents])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import streamlit as st\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# llm = GoogleGenerativeAI(model=\"gemini-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Ensure that the environment variable is set\n",
    "# If \"LANG_API\" is not set, use a default value\n",
    "lang_api_key = os.getenv(\"LANG_API\", \"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\")\n",
    "gem_api_key = os.getenv(\"GEM_API_KEY\", \"AIzaSyABqRZfsB8i5nU1r43ygsLXKie8D8SvqZA\")\n",
    "\n",
    "if lang_api_key is None or gem_api_key is None:\n",
    "    raise ValueError(\"Environment variables 'LANG_API' or 'GEM_API_KEY' are not set.\")\n",
    "\n",
    "# Now set the environment variables\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_API_KEY'] = lang_api_key\n",
    "os.environ['GOOGLE_API_KEY'] = gem_api_key\n",
    "llm = GoogleGenerativeAI(model=\"gemini-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-02 12:51:32.103 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-02 12:51:32.103 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-02 12:51:32.106 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-02 12:51:32.108 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-02 12:51:32.110 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-02 12:51:32.110 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-09-02 12:51:32.110 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are a helpful assistant. Please response to the user queries\"),\n",
    "        (\"user\",\"Question:{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "## streamlit framework\n",
    "\n",
    "st.title('Langchain Demo With GEN API')\n",
    "input_text=st.text_input(\"Search the topic u want\")\n",
    "\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "if input_text:\n",
    "    st.write(chain.invoke({'question':input_text}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
