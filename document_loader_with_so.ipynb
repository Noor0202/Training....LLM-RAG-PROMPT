{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Cleaner With (.txt, .json, .html, .csv) Supporting Extension\n",
    "#### below you can create more concreate class if you wana to add more other Extention like docx and all\n",
    "### Note - Use Document Cleaner class only if you want only content into your RAG PipeLine, cause if you use HTMLCleaner then it will clean all css scrpit it will only leave the content which are shown in screen also remove the empty tag i strongly recommoned you to please go through the README before using this files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract class for cleaning documents\n",
    "class AbstractDocumentCleaner(ABC):\n",
    "    @abstractmethod\n",
    "    def clean_document(self, my_documents: list) -> list:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVCleaner(AbstractDocumentCleaner):\n",
    "    def clean_document(self, my_file:str)->str:\n",
    "        import pandas as pd    \n",
    "\n",
    "        df = pd.read_csv(my_file)\n",
    "\n",
    "        df.columns = df.columns.str.strip()\n",
    "        for col in df.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df[col]):\n",
    "                df[col].fillna(0, inplace=True)\n",
    "            else:\n",
    "                df[col].fillna('', inplace=True)\n",
    "                df[col] = df[col].str.strip()\n",
    "\n",
    "        df = df.drop_duplicates()\n",
    "\n",
    "        df.to_csv(my_file, index=False)\n",
    "        \n",
    "        return f\"CSV file '{my_file}' has been cleaned in place.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLCleaner(AbstractDocumentCleaner):\n",
    "    def clean_document(self, my_file: str) -> str:\n",
    "        from bs4 import BeautifulSoup, Comment\n",
    "        \n",
    "        with open(my_file, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        # Remove comments\n",
    "        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "            comment.extract()\n",
    "\n",
    "        # Remove script and style tags\n",
    "        for tag_name in ['script', 'style']:\n",
    "            for tag in soup.find_all(tag_name):\n",
    "                tag.decompose()\n",
    "\n",
    "        # Remove leading and trailing whitespace from tag contents\n",
    "        for tag in soup.find_all(True):  # True finds all tags\n",
    "            if tag.string:\n",
    "                tag.string = tag.string.strip()\n",
    "\n",
    "        # Remove blank tags or tags with only whitespace\n",
    "        for tag in soup.find_all(True):\n",
    "            if not tag.contents or all(str(content).isspace() for content in tag.contents):\n",
    "                tag.decompose()\n",
    "\n",
    "        # Writing clean data back to the file\n",
    "        with open(my_file, 'w', encoding='utf-8') as file:\n",
    "            file.write(str(soup.prettify()))\n",
    "\n",
    "        return f\"HTML file '{my_file}' has been cleaned in place.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONCleaner(AbstractDocumentCleaner):\n",
    "    def clean_document(self, my_file: str) -> str:\n",
    "        import json\n",
    "        with open(my_file, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # Clean the JSON data\n",
    "        def clean_json(d):\n",
    "            if isinstance(d, dict):\n",
    "                return {k.strip(): clean_json(v) for k, v in d.items()}\n",
    "            elif isinstance(d, list):\n",
    "                return [clean_json(i) for i in d]\n",
    "            elif isinstance(d, str):\n",
    "                return d.strip()\n",
    "            else:\n",
    "                return d\n",
    "\n",
    "        cleaned_data = clean_json(data)\n",
    "\n",
    "        # Writing clean data back to the file\n",
    "        with open(my_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(cleaned_data, file, indent=4, separators=(',', ': '))\n",
    "\n",
    "        return f\"JSON file '{my_file}' has been cleaned in place.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TXTCleaner(AbstractDocumentCleaner):\n",
    "    def clean_document(self, my_file: str) -> str:\n",
    "        with open(my_file, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        cleaned_lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "        with open(my_file, 'w', encoding='utf-8') as file:\n",
    "            file.write('\\n'.join(cleaned_lines))\n",
    "\n",
    "        return f\"Text file '{my_file}' has been cleaned in place.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentCleaner:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.cleaner_mapping = {\n",
    "            '.csv': CSVCleaner(),\n",
    "            '.html': HTMLCleaner(),\n",
    "            '.json': JSONCleaner(),\n",
    "            '.txt': TXTCleaner(),\n",
    "        }\n",
    "\n",
    "    def clean(self, my_file_path_or_api: list):\n",
    "        try:\n",
    "            file_extension = os.path.splitext(my_file_path_or_api)[1].lower()\n",
    "            loader = self.cleaner_mapping[file_extension]\n",
    "            if loader is None:\n",
    "                raise ValueError(f\"Unsupported File type: {file_extension}\")\n",
    "\n",
    "            print(loader.clean_document(my_file_path_or_api))\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(f\"ValueError : {ve}\")\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Either no such file {my_file_path_or_api} or Something error occurred in code : {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loader With (.pdf, .txt, .json, .html, .csv and API) Supporting Extension\n",
    "#### below you can create more concreate class if you wana to add more other Extention like docx and all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract base class for document loader\n",
    "class AbstractDocumentLoader(ABC):\n",
    "    @abstractmethod\n",
    "    def load_document(self, my_file_path_or_api: str):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concrete class for loading CSV data\n",
    "class CSVLoader(AbstractDocumentLoader):\n",
    "    def load_document(self, my_file_path_or_api: str):\n",
    "        from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "        loader = CSVLoader(file_path=my_file_path_or_api)\n",
    "        return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concrete class for loading HTML data\n",
    "class HTMLLoader(AbstractDocumentLoader):\n",
    "    def load_document(self, my_file_path_or_api: str):\n",
    "        from langchain_community.document_loaders import BSHTMLLoader\n",
    "        loader = BSHTMLLoader(my_file_path_or_api)\n",
    "        return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concrete class for loading JSON data\n",
    "class JSONLoader(AbstractDocumentLoader):\n",
    "    def load_document(self, my_file_path_or_api: str):\n",
    "        from langchain_community.document_loaders import JSONLoader\n",
    "        loader = JSONLoader(\n",
    "            file_path=my_file_path_or_api,\n",
    "            jq_schema=\".\",\n",
    "            text_content=False,\n",
    "            json_lines=False\n",
    "        )\n",
    "        return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concrete class for loading PDF data\n",
    "class PDFLoader(AbstractDocumentLoader):\n",
    "    def load_document(self, my_file_path_or_api: str):\n",
    "        from langchain_community.document_loaders import PyPDFLoader\n",
    "        loader = PyPDFLoader(my_file_path_or_api)\n",
    "        return loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concrete class for loading TXT data\n",
    "class TXTLoader(AbstractDocumentLoader):\n",
    "    def load_document(self, my_file_path_or_api: str):\n",
    "        from langchain.document_loaders import TextLoader\n",
    "        loader = TextLoader(my_file_path_or_api)\n",
    "        return loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concrete class for loading API data\n",
    "class APILoader(AbstractDocumentLoader):\n",
    "    def load_document(self, my_api_path: str):\n",
    "        import requests\n",
    "        from langchain.schema import Document\n",
    "        response = requests.get(my_api_path)\n",
    "        api_data = response.json()\n",
    "        return [Document(page_content=str(api_data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below is Main class or we can say a factory class in by using below class we can call above classes with respect to document type, only this loader class will call.\n",
    "\n",
    "#### Note - if you add above new concreate class don't furget to add .extension and its loader class in loader_mapping dict in below, don;t need to change anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentHandler:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.loader_mapping = {\n",
    "            '.csv': CSVLoader(),\n",
    "            '.html': HTMLLoader(),\n",
    "            '.json': JSONLoader(),\n",
    "            '.pdf': PDFLoader(),\n",
    "            '.txt': TXTLoader(),\n",
    "        }\n",
    "\n",
    "    def load(self, my_file_path_or_api: str):\n",
    "        try:\n",
    "            if my_file_path_or_api.startswith(\"http\"):\n",
    "                loader = APILoader()\n",
    "            else:\n",
    "                file_extension = os.path.splitext(my_file_path_or_api)[1].lower()\n",
    "                \n",
    "                if file_extension in [\".c\", \".cpp\", \".py\", \".java\", \".js\", \".ts\", \".cs\", \".rb\", \".php\", \".html\", \".css\", \".swift\", \".go\", \".rs\", \".kt\", \".m\", \".h\", \".sh\", \".pl\", \".r\", \".lua\", \".asm\", \".scala\", \".sql\", \".xml\", \".json\", \".yml\", \".bat\", \".ps1\", \".vb\", \".dart\", \".erl\", \".ex\", \".f\", \".ml\", \".hs\", \".jl\", \".md\", \".rkt\", \".clj\", \".v\", \".vhd\", \".pas\", \".tsx\", \".jsx\"]:\n",
    "                    file_extension = \".txt\"\n",
    "                    \n",
    "                loader = self.loader_mapping[file_extension]\n",
    "                if loader is None:\n",
    "                    raise ValueError(f\"Unsupported document type: {file_extension}\")\n",
    "\n",
    "            return loader.load_document(my_file_path_or_api)\n",
    "\n",
    "        except ValueError as ve:\n",
    "            print(f\"ValueError : {ve}\")\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Either no such file {my_file_path_or_api} or Something error occurred in code : {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use case of above class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "\n",
    "# let below two method i have choice that i can either clean the data or pass as it is without cleaning- \n",
    "document_cleaner = DocumentCleaner()\n",
    "\n",
    "# Uncomment the line which document you wana to clean:\n",
    "# document_cleaner.clean(\"csv data.csv\")  # Cleaning CSV file\n",
    "# document_cleaner.clean(\"html data.html\")  # Cleaning HTML file\n",
    "# document_cleaner.clean(\"json data.json\")  # Cleaning JSON file\n",
    "# document_cleaner.clean(\"txt data.txt\")  # Cleaning TXT file\n",
    "\n",
    "document_handler = DocumentHandler()\n",
    "\n",
    "# Uncomment the line which document you wana to import:\n",
    "# data = document_handler.load(\"csv data.csv\")  # Loading CSV file\n",
    "# data = document_handler.load(\"html data.html\")  # Loading HTML file\n",
    "# data = document_handler.load(\"json data.json\")  # Loading JSON file\n",
    "# data = document_handler.load(\"pdf data.pdf\")  # Loading PDF file\n",
    "# data = document_handler.load(\"txt data.txt\")  # Loading TXT file\n",
    "# data = document_handler.load(\"https://dummy-json.mock.beeceptor.com/todos\")  # Loading API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentLoader_and_Cleaner:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.cleaner_mapping = {\n",
    "            '.csv': CSVCleaner(),\n",
    "            '.html': HTMLCleaner(),\n",
    "            '.json': JSONCleaner(),\n",
    "            '.txt': TXTCleaner(),\n",
    "        }\n",
    "        \n",
    "        self.loader_mapping = {\n",
    "            '.csv': CSVLoader(),\n",
    "            '.html': HTMLLoader(),\n",
    "            '.json': JSONLoader(),\n",
    "            '.pdf': PDFLoader(),\n",
    "            '.txt': TXTLoader(),\n",
    "        }\n",
    "\n",
    "    def clean_and_load(self, my_file_path_or_api: list):\n",
    "        try:\n",
    "            if my_file_path_or_api.startswith(\"http\"):\n",
    "                loader = APILoader()\n",
    "            \n",
    "            else:    \n",
    "                file_extension = os.path.splitext(my_file_path_or_api)[1].lower()\n",
    "                \n",
    "                if file_extension == \".pdf\":\n",
    "                    loader = self.loader_mapping[file_extension]\n",
    "                    return loader.load_document(my_file_path_or_api)\n",
    "                \n",
    "                elif file_extension in [\".c\", \".cpp\", \".py\", \".java\", \".js\", \".ts\", \".cs\", \".rb\", \".php\", \".css\", \".swift\", \".go\", \".rs\", \".kt\", \".m\", \".h\", \".sh\", \".pl\", \".r\", \".lua\", \".asm\", \".scala\", \".sql\", \".xml\", \".yml\", \".bat\", \".ps1\", \".vb\", \".dart\", \".erl\", \".ex\", \".f\", \".ml\", \".hs\", \".jl\", \".md\", \".rkt\", \".clj\", \".v\", \".vhd\", \".pas\", \".tsx\", \".jsx\"]:\n",
    "                    \n",
    "                    file_extension = \".txt\"\n",
    "                \n",
    "                loader = self.cleaner_mapping[file_extension]\n",
    "                print(loader.clean_document(my_file_path_or_api))\n",
    "                loader = self.loader_mapping[file_extension]\n",
    "            \n",
    "            if loader is None:\n",
    "                raise ValueError(f\"Unsupported File type: {file_extension}\")\n",
    "            \n",
    "            return loader.load_document(my_file_path_or_api)\n",
    "            \n",
    "        except ValueError as ve:\n",
    "            print(f\"ValueError : {ve}\")\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Either no such file {my_file_path_or_api} or Something error occurred in code : {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Splitter With (.txt, .json, .html, .csv .pdf and Programming Langucage Extension's) Supporting Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import copy\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, HTMLHeaderTextSplitter, HTMLSectionSplitter,CharacterTextSplitter,Language\n",
    "class Document:\n",
    "    def __init__(self, page_content, metadata):\n",
    "        self.page_content = page_content\n",
    "        self.metadata = metadata\n",
    "\n",
    "class DocumentSplitter:\n",
    "    \n",
    "    def __init__(self, chunk_size, chunk_overlap):\n",
    "        \n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        \n",
    "        self.rcts_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \";\", \",\", \":\", \" \", \"-\", \"_\", \"\\uff0c\", \"\\uff0e\", \"\\u3001\", \"\\u3002\", \"\\u200b\", \"\\u2028\", \"\\u2029\", \"\"],\n",
    "            is_separator_regex=False\n",
    "        )\n",
    "    \n",
    "    def __clean_documents(self, documents):\n",
    "        cleaned_documents = []\n",
    "        for document in documents:\n",
    "            # Ensure page_content is a string and contains unwanted characters\n",
    "            if isinstance(document.page_content, str):\n",
    "                cleaned_text = re.sub(r'\\s+', ' ', document.page_content)  # Replace multiple whitespace characters with a single space\n",
    "                cleaned_text = cleaned_text.strip()  # Remove leading and trailing whitespace\n",
    "                \n",
    "                # Use deep copy to create a new Document object with cleaned content\n",
    "                cleaned_document = copy.deepcopy(document)\n",
    "                cleaned_document.page_content = cleaned_text\n",
    "                cleaned_documents.append(cleaned_document)\n",
    "        return cleaned_documents\n",
    "    \n",
    "    def split_by_rcts(self, documents):\n",
    "        cleaned_documents = self.__clean_documents(documents)\n",
    "        split_documents = []\n",
    "        for document in cleaned_documents:\n",
    "            # Ensure page_content is a string\n",
    "            if isinstance(document.page_content, str):\n",
    "                split_texts = self.rcts_splitter.create_documents([document.page_content])\n",
    "                for split_doc in split_texts:\n",
    "                    split_doc.metadata.update(document.metadata)\n",
    "                    split_documents.append(split_doc)\n",
    "            else:\n",
    "                print(f\"Skipping document with invalid content type: {type(document.page_content)}\")\n",
    "        return split_documents\n",
    "    \n",
    "    def split_by_char(self, documents):\n",
    "        text_splitter = CharacterTextSplitter(\n",
    "            separator=\"\\n\\n\",chunk_size=self.chunk_size,chunk_overlap=self.chunk_overlap,length_function=len,is_separator_regex=False,)\n",
    "        \n",
    "        cleaned_documents = self.__clean_documents(documents)\n",
    "        split_documents = []\n",
    "        for document in cleaned_documents:\n",
    "            # Ensure page_content is a string\n",
    "            if isinstance(document.page_content, str):\n",
    "                split_texts = text_splitter.create_documents([document.page_content])\n",
    "                for split_doc in split_texts:\n",
    "                    split_doc.metadata.update(document.metadata)\n",
    "                    split_documents.append(split_doc)\n",
    "            else:\n",
    "                print(f\"Skipping document with invalid content type: {type(document.page_content)}\")\n",
    "        return split_documents\n",
    "\n",
    "    def split_by_code(self, documents):\n",
    "        \n",
    "        extension = documents[0].metadata[\"source\"].split(\".\")[-1]\n",
    "        language_mapping = {\n",
    "            \"cpp\": Language.CPP, \"go\": Language.GO, \"java\": Language.JAVA,\n",
    "            \"kt\": Language.KOTLIN, \"js\": Language.JS, \"ts\": Language.TS,\n",
    "            \"php\": Language.PHP, \"proto\": Language.PROTO, \"py\": Language.PYTHON,\n",
    "            \"rst\": Language.RST, \"rb\": Language.RUBY, \"rs\": Language.RUST,\n",
    "            \"scala\": Language.SCALA, \"swift\": Language.SWIFT, \"md\": Language.MARKDOWN,\n",
    "            \"tex\": Language.LATEX, \"html\": Language.HTML, \"sol\": Language.SOL,\n",
    "            \"cs\": Language.CSHARP, \"c\": Language.C, \"lua\": Language.LUA,\n",
    "            \"pl\": Language.PERL, \"hs\": Language.HASKELL\n",
    "        }\n",
    "\n",
    "        if extension in language_mapping:\n",
    "            code_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "                language=language_mapping[extension],\n",
    "                chunk_size=self.chunk_size,\n",
    "                chunk_overlap=self.chunk_overlap\n",
    "            )\n",
    "            \n",
    "            split_documents = []\n",
    "            for document in documents:\n",
    "                if isinstance(document.page_content, str):\n",
    "                    split_texts = code_splitter.create_documents([document.page_content])\n",
    "                    for split_doc in split_texts:\n",
    "                        split_doc.metadata.update(document.metadata)\n",
    "                        split_documents.append(split_doc)\n",
    "                else:\n",
    "                    print(f\"Skipping document with invalid content type: {type(document.page_content)}\")\n",
    "            return split_documents\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format for code splitting.\")\n",
    "\n",
    "    def __split_by_html(self, documents, url, my_headers_to_split_on, splitter_class):\n",
    "        \"\"\"Helper function for HTML-based splitting.\"\"\"\n",
    "        html_splitter = splitter_class(headers_to_split_on=my_headers_to_split_on)\n",
    "\n",
    "        if url:\n",
    "            html_splits = html_splitter.split_text_from_url(url)\n",
    "        else:\n",
    "            html_splits = []\n",
    "            for document in documents:\n",
    "                split_texts = html_splitter.split_text(document.page_content)\n",
    "                for split_doc in split_texts:\n",
    "                    split_doc.metadata.update(document.metadata)\n",
    "                    html_splits.append(split_doc)\n",
    "\n",
    "        return self.split_by_rcts(html_splits)\n",
    "\n",
    "    def split_by_html_header(self, documents=None, url=None, my_headers_to_split_on=None):\n",
    "        return self.__split_by_html(documents, url, my_headers_to_split_on, HTMLHeaderTextSplitter)\n",
    "    \n",
    "    def split_by_html_section(self, documents=None, url=None, my_headers_to_split_on=None):\n",
    "        return self.__split_by_html(documents, url, my_headers_to_split_on, HTMLSectionSplitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_splitter = DocumentSplitter(50,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=\"The Impact of Artificial Intelligence on Modern Society  \\nArtificial Intelligence (AI) is transforming various aspects of our daily lives. From smart assistants like \\nSiri and Alexa to advanced algorithms that drive self -driving cars, AI has become an integral part of \\nthe modern world.  \\nIn healthcare, AI is used to analyze medical data, predict patient outcomes, and even assist in \\ncomplex surgeries. In finance, algorithms are employed for fraud detection, investment analysis, and \\nautomated trading. Moreover, AI is revolutionizing industri es such as retail, manufacturing, and \\nlogistics by optimizing supply chains and improving customer experiences.  \\nDespite its advantages, AI also poses challenges, including ethical concerns about privacy, job \\ndisplacement, and decision -making transparency. As AI continues to evolve, it is crucial to address \\nthese issues to ensure that the technology benefits society as a whole.  \\nKey Areas of AI Impact  \\n- Healthcare : Diagnostic tools, predictive analytics, robotic surgery.  \\n- Finance : Fraud detection, investment strategies, automated trading.  \\n- Retail : Personalized recommendations, inventory management, customer service.  \\n- Manufacturing : Predictive maintenance, quality control, automation.  \\nAI's potential is immense, but its deployment must be handled with care to mitigate risks and \\nmaximize benefits.\")]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# but hear i don't have choice in one step i can clean and load the data\n",
    "dlc = DocumentLoader_and_Cleaner()\n",
    "doc_list = dlc.clean_and_load(\"short pdf.pdf\")\n",
    "# print(len(doc_list))\n",
    "doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='The Impact of Artificial Intelligence on Modern'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='on Modern Society Artificial Intelligence (AI) is'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='(AI) is transforming various aspects of our daily'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='our daily lives'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='. From smart assistants like Siri and Alexa to'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='Alexa to advanced algorithms that drive self'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='self -driving cars'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=', AI has become an integral part of the modern'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='modern world'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='. In healthcare'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=', AI is used to analyze medical data'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=', predict patient outcomes'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=', and even assist in complex surgeries'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='. In finance'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=', algorithms are employed for fraud detection'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=', investment analysis, and automated trading'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='. Moreover'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=', AI is revolutionizing industri es such as retail'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=', manufacturing'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=', and logistics by optimizing supply chains and'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='and improving customer experiences'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='. Despite its advantages, AI also poses challenges'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=', including ethical concerns about privacy'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=', job displacement'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=', and decision -making transparency'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='. As AI continues to evolve'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=', it is crucial to address these issues to ensure'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='to ensure that the technology benefits society as'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='as a whole'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='. Key Areas of AI Impact - Healthcare'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=': Diagnostic tools'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=', predictive analytics, robotic surgery'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='. - Finance : Fraud detection'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=', investment strategies, automated trading'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='. - Retail : Personalized recommendations'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=', inventory management, customer service'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='. - Manufacturing : Predictive maintenance'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=', quality control, automation'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=\". AI's potential is immense\"),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content=', but its deployment must be handled with care to'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='care to mitigate risks and maximize benefits'),\n",
       " Document(metadata={'source': 'short pdf.pdf', 'page': 0}, page_content='.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_doc = document_splitter.split_by_rcts(doc_list)\n",
    "print(len(split_doc))\n",
    "split_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import json\n",
    "import uuid\n",
    "from langchain.vectorstores import PGVector\n",
    "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "\n",
    "class DocumentEmbedding:\n",
    "    \n",
    "    def __init__(self, documents):\n",
    "        self.instructor_embeddings = HuggingFaceInstructEmbeddings(\n",
    "            model_name=\"hkunlp/instructor-xl\",\n",
    "            model_kwargs={\"device\": \"cpu\"}\n",
    "        )\n",
    "        self.documents = documents\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        embeddings = []\n",
    "        for document_chunk in self.documents:\n",
    "            embedding = self.instructor_embeddings.embed_documents([document_chunk.page_content])[0]\n",
    "            embedding_data = {\n",
    "                \"id\": str(uuid.uuid4()),\n",
    "                \"content\": document_chunk.page_content,\n",
    "                \"meta\": document_chunk.metadata,\n",
    "                \"embedding\": embedding\n",
    "            }\n",
    "            embeddings.append(embedding_data)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHANOOR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\InstructorEmbedding\\instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHANOOR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\SHANOOR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sentence_transformers\\models\\Dense.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(input_path, 'pytorch_model.bin'), map_location=torch.device('cpu')))\n"
     ]
    }
   ],
   "source": [
    "document_embed = DocumentEmbedding(split_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_embed = document_embed.get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(split_doc))\n",
    "print(type(split_doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileEmbeddingStorage:\n",
    "    \n",
    "    def __init__(self, embeddings, file_path):\n",
    "        self.embeddings = embeddings\n",
    "        self.file_path = file_path\n",
    "    \n",
    "    def store(self):\n",
    "        embeddings_file = os.path.join(self.file_path, 'embeddings.txt')\n",
    "        with open(embeddings_file, 'w') as file:\n",
    "            for embedding_data in self.embeddings:\n",
    "                file.write(json.dumps(embedding_data) + '\\n')\n",
    "        print(f\"Embeddings saved successfully in {embeddings_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_embed_store = FileEmbeddingStorage(get_embed,\"embeddings_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseEmbeddingStorage:\n",
    "    \n",
    "    def __init__(self, embeddings, my_split_doc,instance_class,mydbname=\"Sample_DataBase\", myuser=\"postgres\", mypassword=\"root\", myhost=\"localhost\", myport=\"5432\"):\n",
    "        self.embeddings = embeddings\n",
    "        self.instance = instance_class\n",
    "        self.split_doc = my_split_doc\n",
    "        self.dbname = mydbname\n",
    "        self.user = myuser\n",
    "        self.password = mypassword\n",
    "        self.host = myhost\n",
    "        self.port = myport\n",
    "        self.conn = self._make_db_connection()\n",
    "    \n",
    "    def _make_db_connection(self):\n",
    "        try:\n",
    "            conn = psycopg2.connect(\n",
    "                dbname=self.dbname,  \n",
    "                user=self.user,           \n",
    "                password=self.password,  \n",
    "                host=self.host,          \n",
    "                port=self.port\n",
    "            )\n",
    "            return conn\n",
    "        except psycopg2.OperationalError as e:\n",
    "            print(f\"Error: Could not connect to the database. Reason: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _create_table(self, table_query):\n",
    "        cursor = self.conn.cursor()\n",
    "        cursor.execute(table_query)\n",
    "        self.conn.commit()\n",
    "        cursor.close()\n",
    "\n",
    "    def store(self):\n",
    "        if not self.conn:\n",
    "            print(\"Database connection failed.\")\n",
    "            return\n",
    "\n",
    "        create_json_table_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS embeddings_json (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            meta TEXT,\n",
    "            content TEXT NOT NULL,\n",
    "            embedding JSONB NOT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "        self._create_table(create_json_table_query)\n",
    "\n",
    "        insert_query = \"\"\"\n",
    "        INSERT INTO embeddings_json (id, meta, content, embedding)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "        ON CONFLICT (id) DO NOTHING;\n",
    "        \"\"\"\n",
    "        cursor = self.conn.cursor()\n",
    "        for embedding_data in self.embeddings:\n",
    "            cursor.execute(insert_query, (\n",
    "                embedding_data[\"id\"], \n",
    "                json.dumps(embedding_data[\"meta\"]), \n",
    "                embedding_data[\"content\"], \n",
    "                json.dumps(embedding_data[\"embedding\"])\n",
    "            ))\n",
    "            self.conn.commit()\n",
    "        \n",
    "        cursor.close()\n",
    "        self.conn.close()\n",
    "        print(\"Embeddings inserted into PostgreSQL successfully!\")\n",
    "\n",
    "    def store_in_pgvector(self, my_collection_name):\n",
    "        self.conn = self._make_db_connection()\n",
    "        if not self.conn:\n",
    "            print(\"Database connection failed.\")\n",
    "            return\n",
    "        \n",
    "        create_pgvector_table_query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS embeddings_pgvector (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            meta TEXT,\n",
    "            content TEXT NOT NULL,\n",
    "            embedding vector(768)\n",
    "        );\n",
    "        \"\"\"\n",
    "        self._create_table(create_pgvector_table_query)\n",
    "\n",
    "        CONNECTION_STRING = PGVector.connection_string_from_db_params(\n",
    "            driver=\"psycopg2\",\n",
    "            host=self.host,\n",
    "            port=self.port,\n",
    "            database=self.dbname,\n",
    "            user=self.user,\n",
    "            password=self.password\n",
    "        )\n",
    "        \n",
    "        db = PGVector.from_documents(\n",
    "            embedding=self.instance.instructor_embeddings,\n",
    "            documents= self.split_doc,\n",
    "            collection_name=my_collection_name,\n",
    "            connection_string=CONNECTION_STRING,\n",
    "            pre_delete_collection=True\n",
    "        )\n",
    "\n",
    "        self.conn.close()\n",
    "        print(f\"Embeddings inserted into PGVector collection '{my_collection_name}' successfully!\")\n",
    "        return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_obj = DatabaseEmbeddingStorage(get_embed,split_doc,document_embed,\"Sample_DataBase\",\"postgres\",\"root\",\"localhost\",\"5432\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'id': '22b50637-eddd-439c-9d9d-97bbeab36da1', 'content': 'The Impact of Artificial Intelligence on Modern', 'meta': {'source': 'short pdf.pdf', 'page': 0}, 'embedding': [-0.019584836438298225, 0.00015889934729784727, 0.039740949869155884, -0.05959833040833473, -0.10919196903705597, -0.04749070480465889, -0.0501183345913887, -0.04814533516764641, -0.05852341279387474, -0.06992729753255844, 0.03585956618189812, 0.013958292081952095, -0.034639738500118256, -0.09370286762714386, 0.01548604853451252, -0.03756038472056389, -0.005083034746348858, -0.07230103015899658, -0.01091417670249939, -0.01570998504757881, -0.027006128802895546, 0.02054597996175289, -0.02925829030573368, 0.021313119679689407, 0.004738526418805122, -0.07115960866212845, -0.028102770447731018, 0.025967972353100777, -0.008715696632862091, 0.023442978039383888, 0.011703362688422203, -0.0200587697327137, 0.028940964490175247, -0.019682524725794792, 0.033738572150468826, -0.05096295103430748, 0.02778388187289238, 0.049058713018894196, 0.05437041074037552, -0.05148803070187569, 0.04886399954557419, 0.0025206785649061203, 0.024818338453769684, -0.05919649079442024, 0.03267130255699158, 0.0408545657992363, 0.010822027921676636, -0.024861102923750877, -0.003069031983613968, -0.014697149395942688, 0.017393572255969048, -0.03453778102993965, 0.009689630940556526, -0.07481012493371964, 0.011802119202911854, -0.019316572695970535, -0.027112197130918503, 0.017765427008271217, -0.05910424888134003, 0.06811035424470901, 0.030143704265356064, -0.014694235287606716, 0.01912006549537182, -0.011065828613936901, 0.015114196576178074, 0.02169390767812729, 0.06581351161003113, -0.0089446185156703, -0.026865985244512558, 0.037372712045907974, -0.013143823482096195, 0.0029837980400770903, 0.024304527789354324, -0.01313779316842556, -0.011033675633370876, 0.047242797911167145, 0.09791585057973862, 0.04927925765514374, 0.03676163777709007, -0.025317026302218437, -0.09773112833499908, -0.011675070971250534, -0.05073032155632973, 0.019784187898039818, -0.035031527280807495, -0.009771019220352173, -0.0027924897149205208, -0.024466970935463905, 0.04408734664320946, 0.08124137669801712, -0.011509111151099205, 0.002128586173057556, -0.029128825291991234, -0.019395016133785248, -0.0016940184868872166, -0.018575850874185562, 0.05130228400230408, 0.0033210066612809896, -0.027885206043720245, -0.06901011615991592, 0.002755817025899887, 0.03941243514418602, -0.013849186711013317, -0.033220626413822174, -0.009489402174949646, 0.023225165903568268, -0.003910317085683346, 0.07196271419525146, 0.0027310617733746767, 0.017343085259199142, 0.0497874990105629, 0.06328848004341125, 0.03752012923359871, -0.01089776773005724, 0.05640088766813278, -0.033049486577510834, 0.07161906361579895, -0.002597853308543563, -0.018352078273892403, 0.01191577035933733, 0.035403765738010406, 0.015485988929867744, 0.016458120197057724, 0.04500742629170418, 6.402120925486088e-05, 0.04348301887512207, 0.10824302583932877, -0.011769361793994904, -0.03876419737935066, 0.02597050368785858, 0.079186350107193, -0.052569430321455, 0.02020365744829178, -0.0511438250541687, 0.052836522459983826, -0.03400849923491478, 0.025050343945622444, -0.016579506918787956, 0.013479185290634632, -0.028739528730511665, 0.014823711477220058, -0.02420973777770996, -0.02950899302959442, -0.034743476659059525, -0.026192782446742058, 0.010607763193547726, 0.013706637546420097, 0.035177089273929596, 0.03762349113821983, 0.04671669378876686, 0.002196880057454109, 0.016300735995173454, -0.016566449776291847, -0.006976359523832798, -0.012818360701203346, -0.06030597165226936, -0.013415155932307243, 0.013018698431551456, 0.02716783620417118, -0.037901923060417175, 0.02836810052394867, 0.024410055950284004, -0.060465745627880096, 0.04724805802106857, -0.07439816743135452, 0.008630190044641495, -0.02760627679526806, -0.05747576802968979, -0.003852377412840724, 0.005721514578908682, 0.03212868794798851, -0.015774505212903023, -0.0055730314925313, -0.014611351303756237, 0.04852240905165672, -0.02760613150894642, -0.006172365974634886, 0.017526011914014816, 0.08286874741315842, 0.03309008851647377, -0.029562661424279213, 0.03346113860607147, 0.017006950452923775, 0.037774235010147095, -0.019854679703712463, 0.002806540112942457, -0.01842494122684002, -0.05419512093067169, 0.028387373313307762, 0.054590873420238495, 0.08258964866399765, -0.05029825493693352, -0.006033081095665693, 0.05599183216691017, 0.05719447508454323, -0.010614500381052494, 0.012894636020064354, 0.058458805084228516, -0.037639837712049484, 0.015048090368509293, 0.0026199817657470703, 0.07825712859630585, -0.001407805597409606, 0.01791624166071415, -0.04668232798576355, 0.003355046734213829, -0.004668294917792082, 0.010252340696752071, 0.04203777015209198, -0.0012077782303094864, -0.04294513538479805, 0.01544834766536951, -0.0020308883395045996, -0.032048869878053665, 0.022088168188929558, 0.03479701280593872, -0.015461592935025692, -0.023306971415877342, -0.018678588792681694, 0.01797800324857235, -0.023120705038309097, 0.002406437648460269, 0.03444872051477432, -0.014095011167228222, 0.04956163838505745, 0.03240670636296272, -0.050781089812517166, 0.02532414346933365, 0.04028240591287613, 0.006760859861969948, -0.010317852720618248, -0.026931336149573326, 0.011646270751953125, 0.008623022586107254, -0.01073300838470459, -0.04211459681391716, 0.03491184487938881, -0.05911457911133766, 0.011780462227761745, 0.03183162957429886, 0.019490383565425873, -0.009124941192567348, -0.04727104306221008, -0.04507257416844368, 0.05062977597117424, 0.018977973610162735, -0.022016694769263268, 0.0015287005808204412, -0.046718865633010864, -0.023048920556902885, 0.012841586023569107, -0.01789485663175583, 0.028960706666111946, 0.03972119465470314, -0.040005795657634735, 0.0034590347204357386, 0.014572503045201302, -0.035997964441776276, -0.010413453914225101, 0.045330531895160675, -0.018246421590447426, -0.07213916629552841, -0.02521738037467003, -0.029992520809173584, 0.022983113303780556, -0.009507982060313225, 0.034400418400764465, -0.048607874661684036, 0.04709938168525696, -0.012473149225115776, -0.012437812052667141, 0.05473560467362404, 0.05148940905928612, 0.008441925048828125, 0.04874333366751671, 0.035113003104925156, 0.01132289506494999, -0.052110712975263596, -0.04794078320264816, 0.07292010635137558, -0.002150589134544134, -0.08208116888999939, -0.053394876420497894, 0.01731291599571705, -0.012302663177251816, 0.025629930198192596, 0.028935296460986137, 0.01337607204914093, 0.06065948307514191, -0.010258284397423267, -0.024991163983941078, 0.026647400110960007, 0.015849130228161812, 0.018388383090496063, -0.032203108072280884, 0.00738726370036602, -0.013109294697642326, 0.022150099277496338, 0.07487361133098602, -0.02895207516849041, 0.04704605042934418, 0.0025508631952106953, 0.011486027389764786, -0.04020560905337334, -0.012677881866693497, 0.030756009742617607, -0.07632618397474289, 0.022319940850138664, -0.01258365623652935, 0.02885982021689415, 0.0745169073343277, 0.045724645256996155, 0.042481668293476105, -0.0048853312619030476, -0.013004801236093044, -0.031138280406594276, 0.00556686008349061, -0.02637026645243168, 0.011343834921717644, -0.004196681547909975, 0.037494976073503494, 0.02916845493018627, 0.062268540263175964, -0.024813247844576836, 0.03512058034539223, 0.05156087130308151, 0.04501801356673241, -0.0011010041926056147, 0.05985244736075401, -0.09011507034301758, 0.025618303567171097, 0.07108502835035324, -0.06504860520362854, 0.0600803941488266, -0.016900645568966866, 0.032477788627147675, 0.013093916699290276, -0.005818292032927275, 0.06139260530471802, 0.018740423023700714, -0.008533330634236336, 0.002007220173254609, -0.033312395215034485, 0.020576123148202896, -0.054310284554958344, 0.02978547289967537, 0.02745225466787815, 0.034539371728897095, -0.027555543929338455, -0.02946378104388714, 0.0601484514772892, 0.014655518345534801, -0.03136419132351875, 0.03875403851270676, 0.06017624959349632, -0.02915813773870468, 0.05872628465294838, -0.015738198533654213, -0.041266363114118576, 0.01420358382165432, 0.038372717797756195, 0.0008301359484903514, -0.0003863649908453226, -0.05996512621641159, -0.015646979212760925, 0.01528374757617712, -0.014026283286511898, -0.05544479936361313, 0.05416283756494522, 0.07065299153327942, 0.04158853739500046, 0.004017253872007132, -0.034762877970933914, 0.0095950523391366, -0.030217327177524567, 0.017621872946619987, 0.004527914337813854, -0.04932420328259468, -0.039911359548568726, -0.032108668237924576, -0.001169842784292996, 0.011546110734343529, 0.023497721180319786, 0.018858499825000763, 0.0155287254601717, 0.02415177971124649, 0.02289600670337677, -0.054745811969041824, -0.022142933681607246, -0.012015564367175102, 0.07276753336191177, 0.029466766864061356, 0.0484488382935524, -0.006250586826354265, 0.024806270375847816, -0.05074908211827278, -0.0303655918687582, -0.038646917790174484, -0.041419003158807755, 0.036690060049295425, 0.041960299015045166, -0.052328430116176605, 0.014878938905894756, 0.011700239032506943, 0.06336754560470581, -0.029674511402845383, 0.01348614227026701, 0.07426478713750839, 0.008981196209788322, -0.0006376556702889502, 0.04203147441148758, -0.02360583283007145, -0.021490907296538353, -0.03702166676521301, 0.009993010200560093, 0.04948168620467186, 0.0016303587472066283, -0.009364877827465534, 0.01305356528609991, -0.013634306378662586, -0.012344838120043278, -0.005971843842417002, 0.06014768034219742, -0.011746416799724102, -0.008594137616455555, -0.036494217813014984, 0.02997477352619171, -0.008632443845272064, -0.06532076746225357, 0.018071532249450684, 0.025617530569434166, 0.01410727296024561, -0.02908993698656559, -0.020493192598223686, -0.022979483008384705, 0.013157721608877182, -0.021014973521232605, 0.043200161308050156, -0.06831584870815277, -0.005712225567549467, 0.006856166757643223, -0.028372198343276978, 0.06138288602232933, 0.009424311108887196, 0.039662379771471024, 0.004714386537671089, -0.041471946984529495, -0.04584389179944992, 0.030097099021077156, -0.00877328123897314, 0.03778424486517906, 0.02168854884803295, 0.011904102750122547, 0.06608819216489792, 0.022140976041555405, -0.017505621537566185, -0.007361969910562038, 0.012147707864642143, -0.004019138403236866, -0.026668688282370567, -0.05062579736113548, -0.023596400395035744, -0.025762934237718582, 0.019786393269896507, -0.034179557114839554, -0.008486204780638218, -0.02823656052350998, 0.0013424827484413981, -0.00028452390688471496, -0.03274129331111908, -0.014443696476519108, -0.00497792661190033, 0.044491663575172424, -0.07013080269098282, -0.008593808859586716, -0.028876010328531265, -0.02465159259736538, 0.024316607043147087, 0.0023149470798671246, -0.02550913579761982, -0.05144521966576576, -0.04663084074854851, 0.019453274086117744, -0.02022797428071499, 0.01169192511588335, 0.027266109362244606, -0.0296840388327837, 0.02631261758506298, -0.018974261358380318, 0.019839439541101456, -0.036404553800821304, 0.03205074742436409, 0.041920822113752365, 0.013316153548657894, 0.04055405408143997, -0.04929476976394653, -0.04011106863617897, -0.03976762294769287, 0.019864216446876526, 0.007274676114320755, -0.030964886769652367, -0.016917679458856583, -0.008957423269748688, 0.09236203879117966, -0.06438032537698746, 0.011365721933543682, 0.030683675780892372, -0.008937868289649487, -0.012212755158543587, 0.011346888728439808, 0.06644164025783539, -0.0075902692042291164, -0.031734440475702286, 0.04606002941727638, 0.01107809878885746, -0.01679714024066925, 0.019633788615465164, 0.017959874123334885, 0.007364594377577305, 0.016540907323360443, -0.035084377974271774, 0.011863265186548233, 0.051687296479940414, 0.02371719479560852, -0.03758383169770241, -0.018377238884568214, 0.03405405953526497, -0.03474630415439606, -0.05668018385767937, 0.09940046817064285, -0.025464637205004692, -0.015604335814714432, 0.044178612530231476, 0.006567724980413914, -0.061253439635038376, 0.04055211320519447, 0.027669798582792282, -0.013647357001900673, -0.043409232050180435, 0.024519599974155426, -0.024811794981360435, 0.010818970389664173, -0.020834799855947495, 0.025908414274454117, -0.010442836210131645, -0.005652317311614752, -0.04633348062634468, 0.02219242975115776, -0.04844575375318527, -0.011178631335496902, 0.03320310637354851, 0.00937221571803093, -0.08259119838476181, 0.04592728987336159, -0.015519228763878345, 0.023544687777757645, 0.049874547868967056, -0.013427066616714, 0.006354306824505329, 6.784212018828839e-05, -0.039647653698921204, -0.04979239031672478, -0.07030758261680603, 0.0011237080907449126, -0.024777622893452644, 0.02551904134452343, 0.043260399252176285, -0.054291557520627975, -0.017287040129303932, 0.009607154875993729, 0.055923156440258026, 0.04648752138018608, -0.008727001957595348, -0.006467868573963642, 0.07118383049964905, -0.023204615339636803, 0.011658393777906895, 0.01835000514984131, 0.007939869537949562, 0.044224582612514496, 0.03374654799699783, -0.06788831949234009, -0.0012988088419660926, -0.03419472277164459, -0.08200784027576447, -0.016997266560792923, -0.019296804443001747, 0.018646562471985817, -0.051599424332380295, 0.017157066613435745, 0.0012087691575288773, -0.03496716171503067, -0.024752728641033173, 0.03980013728141785, -0.0033393590711057186, 0.004813844803720713, 0.05674423649907112, -0.04852263256907463, -0.01282635796815157, -0.04401354864239693, 0.02434294857084751, 0.05487029254436493, -0.012775605544447899, 0.004480509087443352, 0.04093494638800621, 0.020226629450917244, 0.007304464466869831, 0.02674458548426628, 0.03370136395096779, -0.019409308210015297, 0.009506868198513985, 0.011952334083616734, 0.08701887726783752, 0.041086044162511826, -0.01331094279885292, 0.04223491623997688, -0.04252650588750839, 0.04188236966729164, -0.007549848407506943, 0.08241353183984756, -0.037704821676015854, 0.037498053163290024, -0.019161812961101532, 0.02264939807355404, 0.007451949175447226, 0.005218772683292627, -0.052949607372283936, -0.01348393689841032, -0.025032684206962585, -0.04148821532726288, -0.01947859115898609, -0.009814459830522537, 0.062268272042274475, 0.021131927147507668, -0.05818403512239456, 0.0002451436303090304, 0.018509922549128532, 0.061915140599012375, 0.005852132569998503, 0.01381090097129345, -0.0044993930496275425, -0.021087054163217545, -0.017246980220079422, -0.01768507808446884, -0.00879994872957468, -0.06913867592811584, 0.016285156831145287, -0.04402785375714302, -0.040671925991773605, 0.007922451943159103, -0.0842251181602478, -0.05065838247537613, -0.009476127102971077, -0.025403445586562157, -0.009833994321525097, -0.024171998724341393, -0.021891653537750244, 0.009008636698126793, 0.006146362982690334, -0.04292464628815651, -0.058044109493494034, 0.04916341230273247, -0.0013490987475961447, 0.008021492511034012, -0.0035692222882062197, 0.01679549179971218, -0.07175761461257935, 0.0025793483946472406, 0.01878713071346283, 0.018677672371268272, -0.005376099608838558, -0.03387453407049179, -0.0012463490711525083, -0.01748717948794365, -0.007003079168498516, -0.026960846036672592, -0.018530530855059624, -0.03539085388183594, -0.056612528860569, -0.00371222710236907, -0.01050445158034563, -0.005051162093877792, -0.006691455841064453, 0.04172850400209427, -0.00598969217389822, -0.05148277059197426, -0.01090208999812603, 0.05894669145345688, -0.051528580486774445, 0.043648481369018555, -0.04222680628299713, -0.01759185642004013, 0.03628019616007805, -0.0056637064553797245, 0.029731523245573044, -0.062222376465797424, 0.024560797959566116, 0.04421225190162659, -0.021697403863072395, 0.011856325902044773, 0.055013347417116165, 0.029157033190131187, 0.00976287666708231, -0.012653917074203491, -0.051737260073423386, -0.02704838663339615, 0.05333379656076431, -0.047023363411426544, 0.001941374852322042, 0.020585771650075912, -0.027123259380459785, -0.007739801425486803, 0.02602454274892807, -0.008631773293018341, 0.007074241992086172, 0.019081009551882744, -0.018446320667862892, -0.00669522350654006, 0.025559525936841965, 0.02069989964365959, 0.019859161227941513, 0.015313372015953064, -0.03372637927532196, 0.06103656440973282, -0.0641234815120697, 0.023220177739858627, 0.005758982617408037, -0.038613710552453995, 0.006332683842629194, -0.03467186912894249, 0.0027220635674893856, -0.010556363500654697, -0.031030697748064995, 0.0018444543238729239, -0.024307535961270332, -0.007318419869989157, 0.055845245718955994, 0.003776993602514267, 0.008633906953036785, 0.04724084213376045, 0.02122444473206997, 0.01170745026320219, 0.02588995359838009, -0.027942271903157234, 0.0020955000072717667, 0.001479212660342455, -0.019876083359122276, 0.06030545383691788, -0.05784176290035248, -0.03527476266026497, -0.05044163763523102, 0.00132263102568686, -0.02204705961048603, -0.002110113622620702, 0.07157009840011597, 0.0020803133957087994, -0.04687727242708206, -0.00829913467168808, 0.08207418769598007, 0.011656387709081173, 0.046511463820934296, 0.024372147396206856, -0.0016901374328881502, 0.03255701810121536, -0.020913323387503624, -0.018892237916588783, -0.0034457948058843613, 0.12230506539344788]}\n"
     ]
    }
   ],
   "source": [
    "print(type(get_embed[0]))\n",
    "print(get_embed[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings inserted into PostgreSQL successfully!\n"
     ]
    }
   ],
   "source": [
    "db_obj.store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHANOOR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_community\\vectorstores\\pgvector.py:487: LangChainPendingDeprecationWarning: Please use JSONB instead of JSON for metadata. This change will allow for more efficient querying that involves filtering based on metadata.Please note that filtering operators have been changed when using JSOB metadata to be prefixed with a $ sign to avoid name collisions with columns. If you're using an existing database, you will need to create adb migration for your metadata column to be JSONB and update your queries to use the new operators. \n",
      "  store = cls(\n",
      "Collection not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings inserted into PGVector collection 'Essay on If i become prime minister' successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.pgvector.PGVector at 0x1b4e38e20f0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_obj.store_in_pgvector(\"Essay on If i become prime minister\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
